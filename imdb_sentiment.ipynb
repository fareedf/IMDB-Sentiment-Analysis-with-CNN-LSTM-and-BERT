{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gz_vL6wwrVO5"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_MON2Zbv7Cw"
      },
      "source": [
        "Part 1: Data Loading & Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXAd3gbN2tLy",
        "outputId": "6a1c35f5-50b1-4acd-875f-694117d2abf3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PART 1: DATA LOADING & PREPROCESSING\n",
            "\n",
            "Dataset loaded: 50000 samples\n",
            "sentiment\n",
            "positive    25000\n",
            "negative    25000\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Train: 35000, Val: 7500, Test: 7500\n",
            "Vocabulary size: 63441\n",
            "Text encoding complete!\n"
          ]
        }
      ],
      "source": [
        "# Part 1: Data Loading & Preprocessing\n",
        "\n",
        "print(\"PART 1: DATA LOADING & PREPROCESSING\")\n",
        "\n",
        "# Load the IMDB dataset\n",
        "df = pd.read_csv('IMDB Dataset.csv')\n",
        "print(f\"\\nDataset loaded: {len(df)} samples\")\n",
        "print(df['sentiment'].value_counts())\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Clean the text by:\n",
        "    - Converting to lowercase\n",
        "    - Removing HTML tags\n",
        "    - Removing punctuation (keeping only letters, numbers, and spaces)\n",
        "    - Removing stopwords\n",
        "    \"\"\"\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<br\\s*/?>', ' ', text)\n",
        "    text = re.sub(r'<[^>]+>', '', text)\n",
        "\n",
        "    # Keep only letters, numbers, and spaces\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
        "\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Remove common stopwords\n",
        "    stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',\n",
        "                 'of', 'with', 'is', 'was', 'are', 'were', 'been', 'be', 'have', 'has',\n",
        "                 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may',\n",
        "                 'might', 'can', 'it', 'its', 'this', 'that', 'these', 'those'}\n",
        "\n",
        "    words = text.split()\n",
        "    words = [w for w in words if w not in stopwords]\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Apply cleaning\n",
        "df['cleaned_review'] = df['review'].apply(clean_text)\n",
        "\n",
        "# Map sentiment to binary values\n",
        "df['label'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n",
        "\n",
        "# Split the dataset: 70/15/15 (train/val/test)\n",
        "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df['label'])\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['label'])\n",
        "\n",
        "print(f\"\\nTrain: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
        "\n",
        "# Build vocabulary from training data\n",
        "class Vocabulary:\n",
        "    def __init__(self, min_freq=2):\n",
        "        self.min_freq = min_freq\n",
        "        self.token2idx = {'<PAD>': 0, '<UNK>': 1}\n",
        "        self.idx2token = {0: '<PAD>', 1: '<UNK>'}\n",
        "\n",
        "    def build_vocab(self, texts):\n",
        "        \"\"\"Build vocabulary from texts\"\"\"\n",
        "        word_counts = Counter()\n",
        "        for text in texts:\n",
        "            tokens = text.split()\n",
        "            word_counts.update(tokens)\n",
        "\n",
        "        idx = 2\n",
        "        for word, count in word_counts.items():\n",
        "            if count >= self.min_freq:\n",
        "                self.token2idx[word] = idx\n",
        "                self.idx2token[idx] = word\n",
        "                idx += 1\n",
        "\n",
        "    def encode(self, text, max_length=100):\n",
        "        \"\"\"Encode text to sequence of indices\"\"\"\n",
        "        tokens = text.split()\n",
        "        indices = [self.token2idx.get(token, 1) for token in tokens]  # 1 is <UNK>\n",
        "\n",
        "        # Pad or truncate\n",
        "        if len(indices) < max_length:\n",
        "            indices = indices + [0] * (max_length - len(indices))  # 0 is <PAD>\n",
        "        else:\n",
        "            indices = indices[:max_length]\n",
        "\n",
        "        return indices\n",
        "\n",
        "# Build vocabulary\n",
        "vocab = Vocabulary(min_freq=2)\n",
        "vocab.build_vocab(train_df['cleaned_review'].values)\n",
        "print(f\"Vocabulary size: {len(vocab.token2idx)}\")\n",
        "\n",
        "# Encode sequences\n",
        "MAX_LENGTH = 100\n",
        "train_df['encoded'] = train_df['cleaned_review'].apply(lambda x: vocab.encode(x, MAX_LENGTH))\n",
        "val_df['encoded'] = val_df['cleaned_review'].apply(lambda x: vocab.encode(x, MAX_LENGTH))\n",
        "test_df['encoded'] = test_df['cleaned_review'].apply(lambda x: vocab.encode(x, MAX_LENGTH))\n",
        "print(\"Text encoding complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxWOjS25v1vO"
      },
      "source": [
        "Part 2: CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSZuEtUt23JX",
        "outputId": "bb1425bb-eb6a-43b0-aebc-48da71657511"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PART 2: CNN MODEL\n",
            "DataLoaders created with batch size: 64\n",
            "\n",
            "CNN Model Architecture:\n",
            "CNNModel(\n",
            "  (embedding): Embedding(63441, 128)\n",
            "  (conv1d): Conv1d(128, 100, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "  (relu): ReLU()\n",
            "  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc): Linear(in_features=5000, out_features=2, bias=True)\n",
            ")\n",
            "\n",
            "Device: cpu\n",
            "\n",
            "Training CNN Model...\n",
            "Epoch 1/3 - Training Loss: 0.5609, Validation Accuracy: 80.33%\n",
            "Epoch 2/3 - Training Loss: 0.3024, Validation Accuracy: 80.53%\n",
            "Epoch 3/3 - Training Loss: 0.1428, Validation Accuracy: 81.69%\n",
            "\n",
            "CNN Test Accuracy: 82.32%\n"
          ]
        }
      ],
      "source": [
        "# Part 2: CNN Model\n",
        "\n",
        "print(\"PART 2: CNN MODEL\")\n",
        "\n",
        "\n",
        "# Custom Dataset\n",
        "class IMDBDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.encodings[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = IMDBDataset(train_df['encoded'].tolist(), train_df['label'].tolist())\n",
        "val_dataset = IMDBDataset(val_df['encoded'].tolist(), val_df['label'].tolist())\n",
        "test_dataset = IMDBDataset(test_df['encoded'].tolist(), test_df['label'].tolist())\n",
        "\n",
        "# Create dataloaders\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"DataLoaders created with batch size: {batch_size}\")\n",
        "\n",
        "# CNN Model\n",
        "class CNNModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=128, num_filters=100, kernel_size=3, num_classes=2):\n",
        "        super(CNNModel, self).__init__()\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # 1D Convolutional layer\n",
        "        # Input: [batch, seq_len, embedding_dim]\n",
        "        # Need to transpose to [batch, embedding_dim, seq_len] for Conv1d\n",
        "        self.conv1d = nn.Conv1d(in_channels=embedding_dim,\n",
        "                                out_channels=num_filters,\n",
        "                                kernel_size=kernel_size,\n",
        "                                padding=kernel_size//2)  # Preserve sequence length\n",
        "\n",
        "        # ReLU activation\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # Max pooling layer\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
        "\n",
        "        # Calculate size after pooling\n",
        "        pooled_length = MAX_LENGTH // 2\n",
        "\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(num_filters * pooled_length, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # Transpose for Conv1d\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        # Convolution\n",
        "        x = self.conv1d(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        # Pooling\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # Flatten\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # Fully connected\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Initialize CNN model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "cnn_model = CNNModel(vocab_size=len(vocab.token2idx), embedding_dim=128, num_filters=100).to(device)\n",
        "\n",
        "print(f\"\\nCNN Model Architecture:\")\n",
        "print(cnn_model)\n",
        "print(f\"\\nDevice: {device}\")\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(cnn_model.parameters(), lr=1e-3)\n",
        "\n",
        "# Training function\n",
        "def train_model(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model(model, data_loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "# Train CNN model\n",
        "print(\"\\nTraining CNN Model...\")\n",
        "num_epochs = 3\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train_model(cnn_model, train_loader, criterion, optimizer, device)\n",
        "    val_accuracy = evaluate_model(cnn_model, val_loader, device)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Training Loss: {train_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "# Test CNN model\n",
        "test_accuracy = evaluate_model(cnn_model, test_loader, device)\n",
        "print(f\"\\nCNN Test Accuracy: {test_accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BDZrrEVvwnE"
      },
      "source": [
        " Part 3: LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUI6tSc04UFn",
        "outputId": "1d24b392-5885-48ed-c46d-8617f67699a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PART 3: LSTM MODEL\n",
            "\n",
            "LSTM Model Architecture:\n",
            "LSTMModel(\n",
            "  (embedding): Embedding(63441, 128)\n",
            "  (lstm): LSTM(128, 128, batch_first=True, bidirectional=True)\n",
            "  (fc): Linear(in_features=256, out_features=2, bias=True)\n",
            ")\n",
            "\n",
            "Training LSTM Model...\n",
            "Epoch 1/3 - Training Loss: 0.5673, Validation Accuracy: 77.97%\n",
            "Epoch 2/3 - Training Loss: 0.3659, Validation Accuracy: 82.92%\n",
            "Epoch 3/3 - Training Loss: 0.2495, Validation Accuracy: 83.77%\n",
            "\n",
            "LSTM Test Accuracy: 84.01%\n"
          ]
        }
      ],
      "source": [
        "# Part 3: LSTM Model\n",
        "\n",
        "print(\"PART 3: LSTM MODEL\")\n",
        "\n",
        "# LSTM Model\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=128, num_classes=2, bidirectional=True):\n",
        "        super(LSTMModel, self).__init__()\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # LSTM layer\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim,\n",
        "                           hidden_size=hidden_dim,\n",
        "                           num_layers=1,\n",
        "                           batch_first=True,\n",
        "                           bidirectional=bidirectional)\n",
        "\n",
        "        # Fully connected layer\n",
        "        # If bidirectional, hidden state is concatenated from both directions\n",
        "        fc_input_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
        "        self.fc = nn.Linear(fc_input_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # LSTM\n",
        "        lstm_out, (hidden, cell) = self.lstm(x)\n",
        "\n",
        "        # Use final hidden state\n",
        "        if self.lstm.bidirectional:\n",
        "            # Concatenate forward and backward hidden states\n",
        "            hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)\n",
        "        else:\n",
        "            hidden = hidden[-1]\n",
        "\n",
        "        # Fully connected\n",
        "        output = self.fc(hidden)\n",
        "\n",
        "        return output\n",
        "\n",
        "# Initialize LSTM model\n",
        "lstm_model = LSTMModel(vocab_size=len(vocab.token2idx),\n",
        "                       embedding_dim=128,\n",
        "                       hidden_dim=128,\n",
        "                       bidirectional=True).to(device)\n",
        "\n",
        "print(f\"\\nLSTM Model Architecture:\")\n",
        "print(lstm_model)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(lstm_model.parameters(), lr=1e-3)\n",
        "\n",
        "# Train LSTM model\n",
        "print(\"\\nTraining LSTM Model...\")\n",
        "num_epochs = 3\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train_model(lstm_model, train_loader, criterion, optimizer, device)\n",
        "    val_accuracy = evaluate_model(lstm_model, val_loader, device)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Training Loss: {train_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "# Test LSTM model\n",
        "test_accuracy = evaluate_model(lstm_model, test_loader, device)\n",
        "print(f\"\\nLSTM Test Accuracy: {test_accuracy:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
